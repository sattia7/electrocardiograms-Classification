{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Electrocardiograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Import the [`electrocardiograms.csv`](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_dataset.csv) dataset and display its first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041199</td>\n",
       "      <td>0.112360</td>\n",
       "      <td>0.146067</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.322097</td>\n",
       "      <td>0.363296</td>\n",
       "      <td>0.413858</td>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.485019</td>\n",
       "      <td>0.511236</td>\n",
       "      <td>0.520599</td>\n",
       "      <td>0.548689</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.606742</td>\n",
       "      <td>0.640449</td>\n",
       "      <td>0.664794</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.780899</td>\n",
       "      <td>0.852060</td>\n",
       "      <td>0.897004</td>\n",
       "      <td>0.953184</td>\n",
       "      <td>0.970037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.985019</td>\n",
       "      <td>0.943820</td>\n",
       "      <td>0.898876</td>\n",
       "      <td>0.823970</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.711610</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.602996</td>\n",
       "      <td>0.576779</td>\n",
       "      <td>0.597378</td>\n",
       "      <td>0.670412</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.513109</td>\n",
       "      <td>0.423221</td>\n",
       "      <td>0.277154</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>0.082397</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.039326</td>\n",
       "      <td>0.054307</td>\n",
       "      <td>0.063670</td>\n",
       "      <td>0.198502</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.355805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.466071</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.305357</td>\n",
       "      <td>0.291071</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.283929</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.260714</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>0.226786</td>\n",
       "      <td>0.217857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.173214</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.141071</td>\n",
       "      <td>0.144643</td>\n",
       "      <td>0.155357</td>\n",
       "      <td>0.167857</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.192857</td>\n",
       "      <td>0.223214</td>\n",
       "      <td>0.251786</td>\n",
       "      <td>0.255357</td>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.310714</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.323214</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.346429</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.341071</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.358929</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.308929</td>\n",
       "      <td>0.360714</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.366071</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.048214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041071</td>\n",
       "      <td>0.101786</td>\n",
       "      <td>0.146429</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.246429</td>\n",
       "      <td>0.301786</td>\n",
       "      <td>0.351786</td>\n",
       "      <td>0.382143</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.398214</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.421429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.951276</td>\n",
       "      <td>0.903712</td>\n",
       "      <td>0.917633</td>\n",
       "      <td>0.900232</td>\n",
       "      <td>0.803944</td>\n",
       "      <td>0.656613</td>\n",
       "      <td>0.421114</td>\n",
       "      <td>0.288863</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.244780</td>\n",
       "      <td>0.230858</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>0.209977</td>\n",
       "      <td>0.206497</td>\n",
       "      <td>0.193735</td>\n",
       "      <td>0.187935</td>\n",
       "      <td>0.179814</td>\n",
       "      <td>0.177494</td>\n",
       "      <td>0.160093</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>0.133411</td>\n",
       "      <td>0.132251</td>\n",
       "      <td>0.121810</td>\n",
       "      <td>0.107889</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>0.113689</td>\n",
       "      <td>0.096288</td>\n",
       "      <td>0.075406</td>\n",
       "      <td>0.066125</td>\n",
       "      <td>0.048724</td>\n",
       "      <td>0.022042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.024362</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.067285</td>\n",
       "      <td>0.112529</td>\n",
       "      <td>0.155452</td>\n",
       "      <td>0.196056</td>\n",
       "      <td>0.220418</td>\n",
       "      <td>0.241299</td>\n",
       "      <td>0.256380</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.262181</td>\n",
       "      <td>0.257541</td>\n",
       "      <td>0.259861</td>\n",
       "      <td>0.261021</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>0.265661</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.263341</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.270302</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.271462</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.273782</td>\n",
       "      <td>0.277262</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.279582</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.278422</td>\n",
       "      <td>0.285383</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.280742</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.286543</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.291183</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.298144</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.303944</td>\n",
       "      <td>0.306264</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.309745</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.317865</td>\n",
       "      <td>0.319026</td>\n",
       "      <td>0.328306</td>\n",
       "      <td>0.341067</td>\n",
       "      <td>0.352668</td>\n",
       "      <td>0.37007</td>\n",
       "      <td>0.390951</td>\n",
       "      <td>0.385151</td>\n",
       "      <td>0.387471</td>\n",
       "      <td>0.37587</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.307425</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.402552</td>\n",
       "      <td>0.62181</td>\n",
       "      <td>0.790023</td>\n",
       "      <td>0.75174</td>\n",
       "      <td>0.468677</td>\n",
       "      <td>0.267981</td>\n",
       "      <td>0.349188</td>\n",
       "      <td>0.356148</td>\n",
       "      <td>0.313225</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.305104</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.302784</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.293503</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.296984</td>\n",
       "      <td>0.300464</td>\n",
       "      <td>0.294664</td>\n",
       "      <td>0.295824</td>\n",
       "      <td>0.301624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984472</td>\n",
       "      <td>0.962733</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.211180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.100932</td>\n",
       "      <td>0.177019</td>\n",
       "      <td>0.270186</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.319876</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.322981</td>\n",
       "      <td>0.330745</td>\n",
       "      <td>0.343168</td>\n",
       "      <td>0.355590</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.411491</td>\n",
       "      <td>0.405280</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.377329</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.361801</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.378882</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.375776</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.374224</td>\n",
       "      <td>0.371118</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.389752</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.439441</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.470497</td>\n",
       "      <td>0.451863</td>\n",
       "      <td>0.459627</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.427019</td>\n",
       "      <td>0.399068</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.354037</td>\n",
       "      <td>0.363354</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.364907</td>\n",
       "      <td>0.388199</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.734472</td>\n",
       "      <td>0.911491</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.614907</td>\n",
       "      <td>0.406832</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>0.349379</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.313665</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.301242</td>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.619217</td>\n",
       "      <td>0.489324</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.108541</td>\n",
       "      <td>0.145907</td>\n",
       "      <td>0.192171</td>\n",
       "      <td>0.222420</td>\n",
       "      <td>0.259786</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.282918</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.275801</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.279359</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.334520</td>\n",
       "      <td>0.357651</td>\n",
       "      <td>0.380783</td>\n",
       "      <td>0.405694</td>\n",
       "      <td>0.435943</td>\n",
       "      <td>0.464413</td>\n",
       "      <td>0.476868</td>\n",
       "      <td>0.491103</td>\n",
       "      <td>0.508897</td>\n",
       "      <td>0.501779</td>\n",
       "      <td>0.496441</td>\n",
       "      <td>0.483986</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.457295</td>\n",
       "      <td>0.430605</td>\n",
       "      <td>0.416370</td>\n",
       "      <td>0.407473</td>\n",
       "      <td>0.386121</td>\n",
       "      <td>0.359431</td>\n",
       "      <td>0.350534</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.341637</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.322064</td>\n",
       "      <td>0.318505</td>\n",
       "      <td>0.313167</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.316726</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.307829</td>\n",
       "      <td>0.304270</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.298932</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.290036</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.288256</td>\n",
       "      <td>0.286477</td>\n",
       "      <td>0.291815</td>\n",
       "      <td>0.297153</td>\n",
       "      <td>0.302491</td>\n",
       "      <td>0.300712</td>\n",
       "      <td>0.309609</td>\n",
       "      <td>0.323843</td>\n",
       "      <td>0.330961</td>\n",
       "      <td>0.327402</td>\n",
       "      <td>0.320285</td>\n",
       "      <td>0.314947</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>0.274021</td>\n",
       "      <td>0.266904</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.261566</td>\n",
       "      <td>0.263345</td>\n",
       "      <td>0.272242</td>\n",
       "      <td>0.277580</td>\n",
       "      <td>0.295374</td>\n",
       "      <td>0.354093</td>\n",
       "      <td>0.471530</td>\n",
       "      <td>0.658363</td>\n",
       "      <td>0.850534</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976868</td>\n",
       "      <td>0.542705</td>\n",
       "      <td>0.193950</td>\n",
       "      <td>0.185053</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.224199</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.209964</td>\n",
       "      <td>0.201068</td>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2       x_3       x_4       x_5       x_6       x_7  \\\n",
       "0  0.000000  0.041199  0.112360  0.146067  0.202247  0.322097  0.363296   \n",
       "1  1.000000  0.901786  0.760714  0.610714  0.466071  0.385714  0.364286   \n",
       "2  0.994200  1.000000  0.951276  0.903712  0.917633  0.900232  0.803944   \n",
       "3  0.984472  0.962733  0.663043  0.211180  0.000000  0.032609  0.100932   \n",
       "4  0.619217  0.489324  0.327402  0.110320  0.000000  0.060498  0.108541   \n",
       "\n",
       "        x_8       x_9      x_10      x_11      x_12      x_13      x_14  \\\n",
       "0  0.413858  0.426966  0.485019  0.511236  0.520599  0.548689  0.599251   \n",
       "1  0.346429  0.314286  0.305357  0.308929  0.305357  0.291071  0.285714   \n",
       "2  0.656613  0.421114  0.288863  0.290023  0.269142  0.244780  0.230858   \n",
       "3  0.177019  0.270186  0.313665  0.307453  0.312112  0.312112  0.313665   \n",
       "4  0.108541  0.145907  0.192171  0.222420  0.259786  0.279359  0.282918   \n",
       "\n",
       "       x_15      x_16      x_17      x_18      x_19      x_20      x_21  \\\n",
       "0  0.606742  0.640449  0.664794  0.730337  0.780899  0.852060  0.897004   \n",
       "1  0.283929  0.271429  0.255357  0.264286  0.260714  0.251786  0.241071   \n",
       "2  0.216937  0.209977  0.206497  0.193735  0.187935  0.179814  0.177494   \n",
       "3  0.315217  0.319876  0.316770  0.312112  0.313665  0.319876  0.316770   \n",
       "4  0.279359  0.275801  0.281139  0.288256  0.286477  0.281139  0.279359   \n",
       "\n",
       "       x_22      x_23      x_24      x_25      x_26      x_27      x_28  \\\n",
       "0  0.953184  0.970037  1.000000  0.992509  0.985019  0.943820  0.898876   \n",
       "1  0.226786  0.217857  0.200000  0.173214  0.164286  0.160714  0.155357   \n",
       "2  0.160093  0.142691  0.133411  0.132251  0.121810  0.107889  0.106729   \n",
       "3  0.307453  0.313665  0.315217  0.315217  0.322981  0.330745  0.343168   \n",
       "4  0.290036  0.291815  0.297153  0.313167  0.334520  0.357651  0.380783   \n",
       "\n",
       "       x_29      x_30      x_31      x_32      x_33      x_34      x_35  \\\n",
       "0  0.823970  0.752809  0.711610  0.666667  0.602996  0.576779  0.597378   \n",
       "1  0.141071  0.144643  0.155357  0.167857  0.175000  0.192857  0.223214   \n",
       "2  0.113689  0.096288  0.075406  0.066125  0.048724  0.022042  0.000000   \n",
       "3  0.355590  0.366460  0.380435  0.394410  0.406832  0.406832  0.411491   \n",
       "4  0.405694  0.435943  0.464413  0.476868  0.491103  0.508897  0.501779   \n",
       "\n",
       "       x_36      x_37      x_38      x_39      x_40      x_41      x_42  \\\n",
       "0  0.670412  0.595506  0.513109  0.423221  0.277154  0.119850  0.082397   \n",
       "1  0.251786  0.255357  0.276786  0.310714  0.323214  0.323214  0.326786   \n",
       "2  0.002320  0.024362  0.046404  0.067285  0.112529  0.155452  0.196056   \n",
       "3  0.405280  0.395963  0.377329  0.377329  0.378882  0.369565  0.363354   \n",
       "4  0.496441  0.483986  0.471530  0.457295  0.430605  0.416370  0.407473   \n",
       "\n",
       "       x_43      x_44      x_45      x_46      x_47      x_48      x_49  \\\n",
       "0  0.022472  0.039326  0.054307  0.063670  0.198502  0.303371  0.355805   \n",
       "1  0.342857  0.346429  0.339286  0.342857  0.348214  0.346429  0.335714   \n",
       "2  0.220418  0.241299  0.256380  0.257541  0.252900  0.257541  0.262181   \n",
       "3  0.366460  0.366460  0.361801  0.357143  0.366460  0.369565  0.363354   \n",
       "4  0.386121  0.359431  0.350534  0.354093  0.341637  0.330961  0.327402   \n",
       "\n",
       "       x_50      x_51      x_52      x_53      x_54      x_55      x_56  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.335714  0.339286  0.341071  0.342857  0.357143  0.358929  0.328571   \n",
       "2  0.257541  0.259861  0.261021  0.269142  0.265661  0.263341  0.263341   \n",
       "3  0.361801  0.366460  0.372671  0.371118  0.369565  0.369565  0.378882   \n",
       "4  0.330961  0.327402  0.313167  0.314947  0.322064  0.318505  0.313167   \n",
       "\n",
       "       x_57      x_58      x_59      x_60      x_61      x_62      x_63  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.308929  0.360714  0.455357  0.457143  0.366071  0.205357  0.114286   \n",
       "2  0.271462  0.270302  0.270302  0.273782  0.278422  0.278422  0.271462   \n",
       "3  0.366460  0.357143  0.371118  0.375776  0.372671  0.364907  0.369565   \n",
       "4  0.311388  0.316726  0.311388  0.302491  0.298932  0.307829  0.304270   \n",
       "\n",
       "       x_64      x_65      x_66      x_67      x_68      x_69      x_70  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.048214  0.000000  0.041071  0.101786  0.146429  0.187500  0.246429   \n",
       "2  0.273782  0.278422  0.279582  0.273782  0.277262  0.279582  0.279582   \n",
       "3  0.374224  0.371118  0.372671  0.380435  0.389752  0.394410  0.408385   \n",
       "4  0.295374  0.290036  0.298932  0.291815  0.290036  0.290036  0.295374   \n",
       "\n",
       "       x_71      x_72      x_73      x_74      x_75      x_76      x_77  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.301786  0.351786  0.382143  0.387500  0.398214  0.407143  0.407143   \n",
       "2  0.278422  0.278422  0.285383  0.283063  0.280742  0.283063  0.287703   \n",
       "3  0.416149  0.439441  0.456522  0.470497  0.451863  0.459627  0.453416   \n",
       "4  0.288256  0.290036  0.288256  0.288256  0.288256  0.286477  0.291815   \n",
       "\n",
       "       x_78      x_79      x_80      x_81      x_82      x_83      x_84  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.410714  0.421429  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.286543  0.283063  0.287703  0.291183  0.293503  0.290023  0.291183   \n",
       "3  0.427019  0.399068  0.394410  0.369565  0.354037  0.363354  0.364907   \n",
       "4  0.297153  0.302491  0.300712  0.309609  0.323843  0.330961  0.327402   \n",
       "\n",
       "       x_85      x_86      x_87      x_88      x_89      x_90      x_91  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.296984  0.294664  0.290023  0.290023  0.295824  0.291183  0.293503   \n",
       "3  0.366460  0.364907  0.388199  0.535714  0.734472  0.911491  1.000000   \n",
       "4  0.320285  0.314947  0.295374  0.281139  0.274021  0.266904  0.263345   \n",
       "\n",
       "       x_92      x_93      x_94      x_95      x_96      x_97      x_98  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.300464  0.301624  0.296984  0.294664  0.300464  0.294664   \n",
       "3  0.919255  0.614907  0.406832  0.372671  0.349379  0.315217  0.304348   \n",
       "4  0.261566  0.263345  0.272242  0.277580  0.295374  0.354093  0.471530   \n",
       "\n",
       "       x_99     x_100     x_101     x_102     x_103     x_104     x_105  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.293503  0.299304  0.303944  0.300464  0.294664  0.299304  0.307425   \n",
       "3  0.313665  0.312112  0.304348  0.298137  0.301242  0.307453  0.298137   \n",
       "4  0.658363  0.850534  1.000000  0.976868  0.542705  0.193950  0.185053   \n",
       "\n",
       "      x_106     x_107     x_108     x_109     x_110     x_111     x_112  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.300464  0.293503  0.303944  0.303944  0.298144  0.296984   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.218861  0.224199  0.201068  0.204626  0.209964  0.201068  0.197509   \n",
       "\n",
       "      x_113     x_114     x_115     x_116     x_117     x_118     x_119  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.303944  0.306264  0.300464  0.301624  0.309745  0.312065  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_120     x_121     x_122     x_123     x_124     x_125     x_126  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.302784  0.310905  0.308585  0.299304  0.301624  0.307425  0.310905   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_127     x_128     x_129     x_130     x_131     x_132     x_133  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.305104  0.308585  0.313225  0.310905  0.309745  0.309745  0.317865   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_134     x_135     x_136     x_137     x_138     x_139     x_140  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.315545  0.310905  0.312065  0.317865  0.319026  0.328306  0.341067   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_141    x_142     x_143     x_144     x_145    x_146     x_147  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "1  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "2  0.352668  0.37007  0.390951  0.385151  0.387471  0.37587  0.338747   \n",
       "3  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "4  0.000000  0.00000  0.000000  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "      x_148     x_149     x_150     x_151     x_152     x_153     x_154  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.312065  0.308585  0.312065  0.307425  0.301624  0.308585  0.307425   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_155     x_156     x_157     x_158    x_159     x_160    x_161  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "2  0.300464  0.283063  0.301624  0.402552  0.62181  0.790023  0.75174   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "\n",
       "      x_162     x_163     x_164     x_165     x_166     x_167     x_168  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.468677  0.267981  0.349188  0.356148  0.313225  0.295824  0.305104   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_169     x_170     x_171     x_172     x_173     x_174     x_175  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.301624  0.302784  0.294664  0.295824  0.300464  0.296984  0.293503   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      x_176     x_177     x_178     x_179     x_180     x_181  x_182  x_183  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "2  0.290023  0.296984  0.300464  0.294664  0.295824  0.301624    0.0    0.0   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0    0.0   \n",
       "\n",
       "   x_184  x_185  x_186  x_187  target  \n",
       "0    0.0    0.0    0.0    0.0       1  \n",
       "1    0.0    0.0    0.0    0.0       1  \n",
       "2    0.0    0.0    0.0    0.0       1  \n",
       "3    0.0    0.0    0.0    0.0       1  \n",
       "4    0.0    0.0    0.0    0.0       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"max_columns\", None)\n",
    "\n",
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_dataset.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’“ Each observation of the dataset is a sequence of measured heartbeats, taken from a patient's electrocardiogram (ECG).\n",
    "\n",
    "ðŸŽ¯ The target is binary and defines whether the heartbeat shows:\n",
    "* a risk of cardiovascular disease ðŸ”´ (1)\n",
    "* or not ðŸŸ¢ (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question** â“\n",
    "\n",
    "Plot an observation of each target class to get a visual idea of what the numbers represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "data['target'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Questions** â“\n",
    "\n",
    "* How many observations of at-risk heartbeats are there? Save your answer as `at_risk_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "at_risk_count=data['target'].value_counts()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many observations of healthy heartbeats are there? Save your answer as `healthy_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "healthy_count=data['target'].value_counts()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘©ðŸ»â€ðŸ« In certain cases, the class balance is representative of the true class distribution. This is the case here: the vast majority of people actually have healthy hearts. In such case, we preserve the class distribution to train the model based on reality, and adapt our modeling approach accordingly.\n",
    "\n",
    "[Centers for Disease Control and Prevention - Heart Disease Facts](https://www.cdc.gov/heartdisease/facts.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/sattia/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/sattia/code/sattia7/data-electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "tests/test_class_balance.py::TestClass_balance::test_at_risk_count \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_class_balance.py::TestClass_balance::test_healthy_count \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/class_balance.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed class_balance step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('class_balance',\n",
    "                         healthy = healthy_count,\n",
    "                         at_risk = at_risk_count)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (3) Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ¯ Your task is to **flag heartbeats that are at risk of cardiovascular diseases.**\n",
    "\n",
    "ðŸ‘‡ Let's start by investigating the performance of a `LogisticRegression` on that task. Use a ***cross-validation to evaluate the model*** on the following metrics:\n",
    "- Accuracy\n",
    "- Recall\n",
    "- Precision\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression()\n",
    "X=data.drop(columns=['target'])\n",
    "y=data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "19560    0\n",
       "19561    0\n",
       "19562    0\n",
       "19563    0\n",
       "19564    0\n",
       "Name: target, Length: 19565, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sattia/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model.fit(X,y)\n",
    "y_pred=model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.94\n",
      "Precision = 0.71\n",
      "Recall = 0.35\n",
      "F1 score = 0.47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "print('Accuracy =', round(accuracy_score(y, y_pred), 2)) # Accuracy\n",
    "\n",
    "print('Precision =', round(precision_score(y, y_pred), 2)) # Precision\n",
    "\n",
    "print('Recall =', round(recall_score(y, y_pred), 2)) # Recall\n",
    "\n",
    "print('F1 score =', round(f1_score(y, y_pred), 2)) # F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([3.4449842 , 2.27971506, 2.28751183, 1.76401067, 3.14065814]),\n",
       " 'score_time': array([0.03002882, 0.01241541, 0.01289439, 0.01635981, 0.01365066]),\n",
       " 'test_score': array([0.29655172, 0.35517241, 0.36206897, 0.32871972, 0.30795848])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "model=LogisticRegression(max_iter=1000)\n",
    "model.fit(X,y)\n",
    "y_pred=model.predict(X)\n",
    "cv_results=cross_validate( model,X,y,cv=5, scoring= 'recall')\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([2.57655931, 2.64374328, 1.69493818, 1.41723037, 1.82393336]),\n",
       " 'score_time': array([0.01720524, 0.0256815 , 0.01669836, 0.02200532, 0.01697564]),\n",
       " 'test_recall': array([0.29655172, 0.35517241, 0.36206897, 0.32871972, 0.30795848]),\n",
       " 'test_precision': array([0.7107438 , 0.66451613, 0.67741935, 0.66901408, 0.712     ]),\n",
       " 'test_accuracy': array([0.93892154, 0.93892154, 0.93994378, 0.93841043, 0.93968822])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results=cross_validate( model,X,y,cv=5, scoring= ['recall', 'precision', 'accuracy'])\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ratio of correct predictions)** â“ \n",
    "\n",
    "What is the ratio of correct predictions for this model ? Save your answer under variable name `correct_pred_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred_ratio=((y==y_pred).value_counts()[1])/(y.count())\n",
    "correct_pred_ratio=round(accuracy_score(y, y_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ability to flag at-risk patients)** â“ \n",
    "\n",
    "What percentage of at-risk heartbeats is the model able to flag? Save your answer under variable name `flag_ratio`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tp/tP+fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "flag_ratio=(((y_pred==1).sum())/y.count())\n",
    "flag_ratio= round(recall_score(y, y_pred), 2) # Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Ability to flag correctly)** â“ \n",
    "\n",
    "When the model signals an at-risk heartbeat, how often is it correct? Save your answer under variable name `correct_detection_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "correct_detection_ratio=(y_pred==1).sum()/(y==1).sum()\n",
    "correct_detection_ratio=round(precision_score(y, y_pred), 2) # Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Detecting as many at-risk patients as possible without too many false alarms)** â“ \n",
    "\n",
    "What is the model's ability to flag as many at-risk heartbeats as possible while limiting false alarms?  Save your answer under variable name `aggregated_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "aggregated_metric=round(f1_score(y, y_pred), 2) # F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/sattia/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/sattia/code/sattia7/data-electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_accuracy \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_f1 \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "tests/test_logistic_regression_evaluation.py::TestLogistic_regression_evaluation::test_recall \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.24s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/logistic_regression_evaluation.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed logistic_regression_evaluation step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('logistic_regression_evaluation',\n",
    "                         accuracy = correct_pred_ratio,\n",
    "                         recall = flag_ratio,\n",
    "                         precision = correct_detection_ratio,\n",
    "                         f1 = aggregated_metric)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â–¶ï¸ Run the following cell before moving on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should have noticed that the model was able to predict correctly in 94 cases out of 100. \n",
      "However, it was able to capture only 35.0 % of the at-risk patients\n",
      "Why ? Let's print a confusion matrix!\n"
     ]
    }
   ],
   "source": [
    "print(f\"You should have noticed that the model was able to predict correctly in {int(round(correct_pred_ratio,2)*100)} cases out of 100. \")\n",
    "\n",
    "print(f\"However, it was able to capture only {round(flag_ratio,2)*100} % of the at-risk patients\")\n",
    "\n",
    "print(\"Why ? Let's print a confusion matrix!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‡ Using `plot_confusion_matrix` ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)),  visualize the predictions breakdown of the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ’¡ Hints</summary>\n",
    "\n",
    "    \n",
    "1. [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)    \n",
    "2. ['ConfusionMatrixDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html)\n",
    "    \n",
    "- Don't forget to to go back to the **Holdout method** to [`train-test-split`]((https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) your dataset and look at the confusion matrix on the test set.  \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f9384662190>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cm=confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=model.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ The confusion matrix should show that the model is influenced by the class imbalance: it predicts the heartbeats to be healthy most of the time. Due to this behaviour, the model is often correct and has a **high accuracy**. However, this also causes it to miss out on many at-risk heartbeats: it has **bad recall**...\n",
    "\n",
    "ðŸ‘‰ This model is therefore poor at the task of **flagging at-risk observations**.\n",
    "\n",
    "â—ï¸ Don't be fooled by the accuracy and look at the metric that corresponds to your task! â—ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Classification Model Selection)** â“ \n",
    "\n",
    "Would a default KNN classifier perform better at the task of flagging at-risk observations?\n",
    "\n",
    "Save the you answer under `best_model` as \"KNN\" or \"LogisticRegression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.99\n",
      "Precision = 0.97\n",
      "Recall = 0.91\n",
      "F1 score = 0.94\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model=KNeighborsClassifier()\n",
    "model.fit(X,y)\n",
    "y_pred=model.predict(X)\n",
    "print('Accuracy =', round(accuracy_score(y, y_pred), 2)) # Accuracy\n",
    "\n",
    "print('Precision =', round(precision_score(y, y_pred), 2)) # Precision\n",
    "\n",
    "print('Recall =', round(recall_score(y, y_pred), 2)) # Recall\n",
    "\n",
    "print('F1 score =', round(f1_score(y, y_pred), 2)) # F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.1578908 , 0.01201272, 0.01260495, 0.01373816, 0.02247643]),\n",
       " 'score_time': array([2.39129257, 2.3606534 , 2.57227063, 2.6772275 , 2.77720261]),\n",
       " 'test_recall': array([0.87241379, 0.84482759, 0.87586207, 0.8615917 , 0.83391003]),\n",
       " 'test_precision': array([0.96197719, 0.93869732, 0.9270073 , 0.93609023, 0.93774319]),\n",
       " 'test_accuracy': array([0.98798876, 0.98441094, 0.98568873, 0.98543317, 0.98364426])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results_KN=cross_validate( model,X,y,cv=5, scoring= ['recall', 'precision', 'accuracy'])\n",
    "cv_results_KN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’ª For this ECG dataset, the KNN Classifier should have a much higher recall than the LogisticRegression and therefore is better suited for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model='KNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/sattia/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/sattia/code/sattia7/data-electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_best_model.py::TestBest_model::test_best_model \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/best_model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed best_model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('best_model',\n",
    "                         model = best_model)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the KNN model thanks to its higherbest recall, let's have a look at the other classification performance metrics>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Classification Report)** â“\n",
    "\n",
    "Print out a [`classification_report`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) of the KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> ðŸ’¡ <i>Hint</i>  </summary>\n",
    "    \n",
    "* You will need to pass the predictions of the model to a `classification_report`.\n",
    "    \n",
    "* SkLearn's [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) might help ðŸ˜‰\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     18117\n",
      "           1       0.97      0.91      0.94      1448\n",
      "\n",
      "    accuracy                           0.99     19565\n",
      "   macro avg       0.98      0.95      0.97     19565\n",
      "weighted avg       0.99      0.99      0.99     19565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Reading the report)** â“\n",
    "\n",
    "\n",
    "Among the heartbeats predicted at-risk, what is the ratio of correct predictions ? \n",
    "\n",
    "In mathematical terms, can you read the ratio $ \\frac{TP}{TP + FP} $ in the report? What is the name of this classification metrics ? \n",
    "\n",
    "Save your answer as a float under `correct_at_risk_predictions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "correct_at_risk_predictions=0.97      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/sattia/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/sattia/code/sattia7/data-electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_precision.py::TestPrecision::test_precision \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/precision.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed precision step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('precision',\n",
    "                         precision = correct_at_risk_predictions)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (7) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â“ **Question (Predicting)** â“\n",
    "\n",
    "A patient comes to you for a second opinion because  he was told that based on his heartbeats, this patient may be at-risk.  \n",
    "\n",
    "According to your optimal model, is he at-risk or not?  \n",
    "\n",
    "Save the prediction of your model under variable name `prediction` as \"at risk\" or \"healthy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>x_11</th>\n",
       "      <th>x_12</th>\n",
       "      <th>x_13</th>\n",
       "      <th>x_14</th>\n",
       "      <th>x_15</th>\n",
       "      <th>x_16</th>\n",
       "      <th>x_17</th>\n",
       "      <th>x_18</th>\n",
       "      <th>x_19</th>\n",
       "      <th>x_20</th>\n",
       "      <th>x_21</th>\n",
       "      <th>x_22</th>\n",
       "      <th>x_23</th>\n",
       "      <th>x_24</th>\n",
       "      <th>x_25</th>\n",
       "      <th>x_26</th>\n",
       "      <th>x_27</th>\n",
       "      <th>x_28</th>\n",
       "      <th>x_29</th>\n",
       "      <th>x_30</th>\n",
       "      <th>x_31</th>\n",
       "      <th>x_32</th>\n",
       "      <th>x_33</th>\n",
       "      <th>x_34</th>\n",
       "      <th>x_35</th>\n",
       "      <th>x_36</th>\n",
       "      <th>x_37</th>\n",
       "      <th>x_38</th>\n",
       "      <th>x_39</th>\n",
       "      <th>x_40</th>\n",
       "      <th>x_41</th>\n",
       "      <th>x_42</th>\n",
       "      <th>x_43</th>\n",
       "      <th>x_44</th>\n",
       "      <th>x_45</th>\n",
       "      <th>x_46</th>\n",
       "      <th>x_47</th>\n",
       "      <th>x_48</th>\n",
       "      <th>x_49</th>\n",
       "      <th>x_50</th>\n",
       "      <th>x_51</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>x_61</th>\n",
       "      <th>x_62</th>\n",
       "      <th>x_63</th>\n",
       "      <th>x_64</th>\n",
       "      <th>x_65</th>\n",
       "      <th>x_66</th>\n",
       "      <th>x_67</th>\n",
       "      <th>x_68</th>\n",
       "      <th>x_69</th>\n",
       "      <th>x_70</th>\n",
       "      <th>x_71</th>\n",
       "      <th>x_72</th>\n",
       "      <th>x_73</th>\n",
       "      <th>x_74</th>\n",
       "      <th>x_75</th>\n",
       "      <th>x_76</th>\n",
       "      <th>x_77</th>\n",
       "      <th>x_78</th>\n",
       "      <th>x_79</th>\n",
       "      <th>x_80</th>\n",
       "      <th>x_81</th>\n",
       "      <th>x_82</th>\n",
       "      <th>x_83</th>\n",
       "      <th>x_84</th>\n",
       "      <th>x_85</th>\n",
       "      <th>x_86</th>\n",
       "      <th>x_87</th>\n",
       "      <th>x_88</th>\n",
       "      <th>x_89</th>\n",
       "      <th>x_90</th>\n",
       "      <th>x_91</th>\n",
       "      <th>x_92</th>\n",
       "      <th>x_93</th>\n",
       "      <th>x_94</th>\n",
       "      <th>x_95</th>\n",
       "      <th>x_96</th>\n",
       "      <th>x_97</th>\n",
       "      <th>x_98</th>\n",
       "      <th>x_99</th>\n",
       "      <th>x_100</th>\n",
       "      <th>x_101</th>\n",
       "      <th>x_102</th>\n",
       "      <th>x_103</th>\n",
       "      <th>x_104</th>\n",
       "      <th>x_105</th>\n",
       "      <th>x_106</th>\n",
       "      <th>x_107</th>\n",
       "      <th>x_108</th>\n",
       "      <th>x_109</th>\n",
       "      <th>x_110</th>\n",
       "      <th>x_111</th>\n",
       "      <th>x_112</th>\n",
       "      <th>x_113</th>\n",
       "      <th>x_114</th>\n",
       "      <th>x_115</th>\n",
       "      <th>x_116</th>\n",
       "      <th>x_117</th>\n",
       "      <th>x_118</th>\n",
       "      <th>x_119</th>\n",
       "      <th>x_120</th>\n",
       "      <th>x_121</th>\n",
       "      <th>x_122</th>\n",
       "      <th>x_123</th>\n",
       "      <th>x_124</th>\n",
       "      <th>x_125</th>\n",
       "      <th>x_126</th>\n",
       "      <th>x_127</th>\n",
       "      <th>x_128</th>\n",
       "      <th>x_129</th>\n",
       "      <th>x_130</th>\n",
       "      <th>x_131</th>\n",
       "      <th>x_132</th>\n",
       "      <th>x_133</th>\n",
       "      <th>x_134</th>\n",
       "      <th>x_135</th>\n",
       "      <th>x_136</th>\n",
       "      <th>x_137</th>\n",
       "      <th>x_138</th>\n",
       "      <th>x_139</th>\n",
       "      <th>x_140</th>\n",
       "      <th>x_141</th>\n",
       "      <th>x_142</th>\n",
       "      <th>x_143</th>\n",
       "      <th>x_144</th>\n",
       "      <th>x_145</th>\n",
       "      <th>x_146</th>\n",
       "      <th>x_147</th>\n",
       "      <th>x_148</th>\n",
       "      <th>x_149</th>\n",
       "      <th>x_150</th>\n",
       "      <th>x_151</th>\n",
       "      <th>x_152</th>\n",
       "      <th>x_153</th>\n",
       "      <th>x_154</th>\n",
       "      <th>x_155</th>\n",
       "      <th>x_156</th>\n",
       "      <th>x_157</th>\n",
       "      <th>x_158</th>\n",
       "      <th>x_159</th>\n",
       "      <th>x_160</th>\n",
       "      <th>x_161</th>\n",
       "      <th>x_162</th>\n",
       "      <th>x_163</th>\n",
       "      <th>x_164</th>\n",
       "      <th>x_165</th>\n",
       "      <th>x_166</th>\n",
       "      <th>x_167</th>\n",
       "      <th>x_168</th>\n",
       "      <th>x_169</th>\n",
       "      <th>x_170</th>\n",
       "      <th>x_171</th>\n",
       "      <th>x_172</th>\n",
       "      <th>x_173</th>\n",
       "      <th>x_174</th>\n",
       "      <th>x_175</th>\n",
       "      <th>x_176</th>\n",
       "      <th>x_177</th>\n",
       "      <th>x_178</th>\n",
       "      <th>x_179</th>\n",
       "      <th>x_180</th>\n",
       "      <th>x_181</th>\n",
       "      <th>x_182</th>\n",
       "      <th>x_183</th>\n",
       "      <th>x_184</th>\n",
       "      <th>x_185</th>\n",
       "      <th>x_186</th>\n",
       "      <th>x_187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.993197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956916</td>\n",
       "      <td>0.902494</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.709751</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.192744</td>\n",
       "      <td>0.147392</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.099773</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.00907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011338</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.054422</td>\n",
       "      <td>0.092971</td>\n",
       "      <td>0.113379</td>\n",
       "      <td>0.160998</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.208617</td>\n",
       "      <td>0.219955</td>\n",
       "      <td>0.240363</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.226757</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.235828</td>\n",
       "      <td>0.24263</td>\n",
       "      <td>0.249433</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.263039</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.276644</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.303855</td>\n",
       "      <td>0.321995</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.340136</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.297052</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>0.274376</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.260771</td>\n",
       "      <td>0.371882</td>\n",
       "      <td>0.639456</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.29932</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.251701</td>\n",
       "      <td>0.256236</td>\n",
       "      <td>0.247166</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        x_1       x_2  x_3       x_4       x_5       x_6       x_7       x_8  \\\n",
       "0  0.904762  0.993197  1.0  0.956916  0.902494  0.857143  0.802721  0.777778   \n",
       "\n",
       "        x_9      x_10      x_11      x_12      x_13      x_14      x_15  \\\n",
       "0  0.709751  0.557823  0.321995  0.192744  0.147392  0.129252  0.099773   \n",
       "\n",
       "       x_16      x_17      x_18      x_19      x_20      x_21      x_22  \\\n",
       "0  0.092971  0.068027  0.068027  0.061224  0.040816  0.034014  0.027211   \n",
       "\n",
       "       x_23     x_24  x_25      x_26  x_27      x_28      x_29      x_30  \\\n",
       "0  0.013605  0.00907   0.0  0.006803   0.0  0.011338  0.015873  0.031746   \n",
       "\n",
       "       x_31      x_32      x_33      x_34      x_35      x_36      x_37  \\\n",
       "0  0.054422  0.092971  0.113379  0.160998  0.185941  0.208617  0.219955   \n",
       "\n",
       "       x_38      x_39      x_40      x_41      x_42      x_43      x_44  \\\n",
       "0  0.240363  0.231293  0.226757  0.231293  0.238095  0.235828  0.235828   \n",
       "\n",
       "      x_45      x_46      x_47      x_48      x_49      x_50      x_51  \\\n",
       "0  0.24263  0.249433  0.253968  0.258503  0.258503  0.256236  0.253968   \n",
       "\n",
       "       x_52      x_53      x_54      x_55      x_56      x_57      x_58  \\\n",
       "0  0.265306  0.263039  0.272109  0.265306  0.260771  0.263039  0.267574   \n",
       "\n",
       "       x_59      x_60      x_61      x_62      x_63      x_64      x_65  \\\n",
       "0  0.267574  0.274376  0.258503  0.265306  0.263039  0.267574  0.272109   \n",
       "\n",
       "       x_66      x_67      x_68      x_69      x_70      x_71      x_72  \\\n",
       "0  0.263039  0.260771  0.274376  0.269841  0.274376  0.276644  0.269841   \n",
       "\n",
       "       x_73      x_74      x_75      x_76      x_77      x_78      x_79  \\\n",
       "0  0.267574  0.274376  0.292517  0.303855  0.321995  0.337868  0.337868   \n",
       "\n",
       "       x_80      x_81      x_82      x_83      x_84      x_85      x_86  \\\n",
       "0  0.340136  0.319728  0.297052  0.285714  0.269841  0.269841  0.274376   \n",
       "\n",
       "       x_87      x_88      x_89      x_90      x_91      x_92      x_93  \\\n",
       "0  0.269841  0.274376  0.267574  0.260771  0.371882  0.639456  0.959184   \n",
       "\n",
       "       x_94      x_95     x_96      x_97      x_98      x_99     x_100  \\\n",
       "0  0.807256  0.444444  0.29932  0.272109  0.278912  0.253968  0.258503   \n",
       "\n",
       "      x_101     x_102     x_103     x_104     x_105     x_106  x_107  x_108  \\\n",
       "0  0.251701  0.256236  0.247166  0.265306  0.265306  0.267574    0.0    0.0   \n",
       "\n",
       "   x_109  x_110  x_111  x_112  x_113  x_114  x_115  x_116  x_117  x_118  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_119  x_120  x_121  x_122  x_123  x_124  x_125  x_126  x_127  x_128  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_129  x_130  x_131  x_132  x_133  x_134  x_135  x_136  x_137  x_138  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_139  x_140  x_141  x_142  x_143  x_144  x_145  x_146  x_147  x_148  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_149  x_150  x_151  x_152  x_153  x_154  x_155  x_156  x_157  x_158  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_159  x_160  x_161  x_162  x_163  x_164  x_165  x_166  x_167  x_168  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_169  x_170  x_171  x_172  x_173  x_174  x_175  x_176  x_177  x_178  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   x_179  x_180  x_181  x_182  x_183  x_184  x_185  x_186  x_187  \n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_patient = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Electrocardiograms_new_patient.csv')\n",
    "new_patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "predictions=model.predict(new_patient)[0]\n",
    "prediction='at risk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ§ª **Check your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.12, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/sattia/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/sattia/code/sattia7/data-electrocardiograms\n",
      "plugins: anyio-3.4.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "tests/test_prediction.py::TestPrediction::test_prediction_at_risk \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "ðŸ’¯ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/prediction.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed prediction step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('prediction',\n",
    "                         prediction = prediction)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ Congratulations!\n",
    "\n",
    "ðŸ’¾ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "ðŸš€ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
